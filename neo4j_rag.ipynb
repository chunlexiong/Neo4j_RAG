{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2282665-89e4-4883-9340-c061d2fb1dc9",
   "metadata": {},
   "source": [
    "# Using Open Source LLMs and a Knowledge Graph to Implement a RAG application\n",
    "\n",
    "## Background\n",
    "Retrieval Augmented Generation (RAG) has become the hottest thing in AI. This comes as no surprise since RAG requires minimal code and helps build user trust in using LLM. The challenge when building a great RAG app or chatbot is handling structured text alongside unstructured text. Knowledge graphs can store both structured and unstructured text within a single database, reducing the work required to give LLM the information it needs. [Neo4j](https://neo4j.com/developer-blog/knowledge-graph-rag-application/) has published a blog demonstrating how to use neo4j-based knowledge graph and OpenAI LLM to build a chatbot to answer questions about microservices architecture and ongoing tasks. Since OpenAI LLM is not open source and its API cannot be used for experimenting free of charge anymore, here I will show you have to use the open source LLMs llama-3.1 to do similar work. You can try with llama-3.0 too, but the performance is much worse.\n",
    "\n",
    "## Neo4j Environment setup\n",
    "First, you'll need to follow the instruction in the [neo4j blog](https://neo4j.com/developer-blog/knowledge-graph-rag-application/) to set up a Neo4j 5.11 instance, or greater. The easiest way is to start a free cloud instance of the Neo4j database on [Neo4j Aura](https://neo4j.com/cloud/platform/aura-graph-database/): use your Gmail or other email address to sign in and choose the free tier plan. The Neo4j cloud service will pop up a window with the username (usually it is 'neo4j') and password (it will remind you this is the only chance to save the password somewhere for later use). It will take a few minutes to start the free cloud instance of the Neo4j database. Once it is started, copy the URI connection to the code below as the value of url. At the same time, use your username and password as well. If you haven't installed the langchain_community python package, you probably want to create a virtual environment for this project and pip install langchain_community before running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ff9c7c3-7c21-46a4-afab-5df23b3fc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "url = \"Your neo4j instance URL\"\n",
    "username =\"neo4j\"\n",
    "password = \"Your neo4j instance password\"\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dfe0e1-83ee-4277-b121-e23d98422390",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "For the purpose of this demo, I will use the same dataset as used in the neo4j blog so that we can easily compare the performance of the open source LLMs and Open AI LLM. In reality, knowledge graphs are excellent at connecting information from multiple data sources. When developing a DevOps RAG application, you can fetch information from cloud services, task management tools, and more.\n",
    "\n",
    "This synthetic dataset is a small dataset with only 100 nodes, but enough for this demo. The following code will import the sample graph into Neo4j instance we started above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b24b5503-7564-480a-828c-688787fe5bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import_url = \"https://gist.githubusercontent.com/tomasonjo/08dc8ba0e19d592c4c3cde40dd6abcc3/raw/e90b0c9386bf8be15b199e8ac8f83fc265a2ac57/microservices.json\"\n",
    "import_query = requests.get(import_url).json()['query']\n",
    "graph.query(\n",
    "    import_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ab5d1-a7d9-4a9a-9d83-a5564a774bfb",
   "metadata": {},
   "source": [
    "Once the graph is imported,  click the 'Open' button in the interface of the started cloud Neo4j service, you will be able open Neo4j browser and see the nodes, relationships and visualisation of the graph.\n",
    "\n",
    "## Neo4j Vector Index\n",
    "We’ll begin with a simpler job by implementing a vector index search to find relevant tasks by their name and description. If you’re unfamiliar with vector similarity search, here’s a quick refresher. The key idea is to calculate the text embedding values for each task based on their description and name. Then, at query time, find the most similar tasks to the user input using a similarity metric like a cosine distance. The retrieved information from the vector index can then be used as context to the LLM so it can generate accurate and up-to-date answers.\n",
    "\n",
    "The tasks are already in our knowledge graph. However, we must calculate the embedding values and create the vector index. Here, we’ll use the from_existing_graph method. Before running the code below, you need to follow the instruction in [OllamaEmbeddings](https://python.langchain.com/v0.2/docs/integrations/text_embedding/ollama/) to download and install Ollama onto the available supported platforms. Once it is installed successfully, fetch available LLM model via running Ollama pull <name-of-model> from terminal and then run Ollama serve. You also need to pip install langchain_ollama.\n",
    "\n",
    "In this example, we use the following graph-specific parameters for the from_existing_graph method.\n",
    "* index_name: name of the vector index.\n",
    "* node_label: node label of relevant nodes.\n",
    "* text_node_properties: properties to be used to calculate embeddings and retrieve from the vector index.\n",
    "* embedding_node_property: which property to store the embedding values to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9007a75c-4831-4b2c-9957-2dfdd8d4e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding = OllamaEmbeddings(model=\"llama3\"),\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name='tasks',\n",
    "    node_label=\"Task\",\n",
    "    text_node_properties=['name', 'description', 'status'],\n",
    "    embedding_node_property='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996abc2f-5078-4fb1-af8a-cbf56f5836cc",
   "metadata": {},
   "source": [
    "Now that the vector index is initiated, we can use it as any other vector index in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3da3323-72d3-4908-bc82-55f3ada0e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = vector_index.similarity_search(\n",
    "    \"How will RecommendationService be updated?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d491c80-b995-4305-a719-a64d022abbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: Update\n",
      "description: Update InventoryService to include real-time stock updates, ensuring accurate reflection of the inventory levels and aiding in the efficient management of stock.\n",
      "status: in progress\n",
      "\n",
      "name: RecommendationFeature\n",
      "description: Add a new feature to RecommendationService to provide more personalized and accurate product recommendations to the users, leveraging user behavior and preference data.\n",
      "status: in progress\n",
      "\n",
      "name: FeatureAdd\n",
      "description: Implement a new feature in OrderService to facilitate bulk orders, ensuring the features seamless integration with existing functionalities and maintaining the overall stability and performance of the service.\n",
      "status: in progress\n",
      "\n",
      "name: Refactor\n",
      "description: Refactor the UserService codebase to enhance its readability, maintainability, and scalability, focusing primarily on modularization and optimization of existing functionalities.\n",
      "status: completed\n"
     ]
    }
   ],
   "source": [
    "for i in response:\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73851431-dbd3-48f0-b884-b81b47d333db",
   "metadata": {},
   "source": [
    "We can see that we construct a response of a map or dictionary-like string with defined properties in the text_node_properties parameter.\n",
    "\n",
    "Now, we can easily create a chatbot response by wrapping the vector index into a RetrievalQA module. Before doing so, you need to create a free account at [groqcloud](https://console.groq.com/) and create an API key. You also need to pip install langchain_groq and insert your own GROQ_API_KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "964608f4-0ce6-4484-90e1-b5c2df757273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "os.environ['GROQ_API_KEY'] = 'Your Groq API key'\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatGroq(model_name = 'llama-3.1-70b-versatile'), chain_type=\"stuff\", retriever=vector_index.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e02db5e-e4c1-48dd-882b-bb0f4898836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How will recommendation service be updated?',\n",
       " 'result': 'The Recommendation Service will be updated to provide more personalized and accurate product recommendations to users, leveraging user behavior and preference data.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_qa.invoke(\n",
    "    {\"query\": \"How will recommendation service be updated?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ce38a-fbb9-40fa-9f25-fb5eb5dd6019",
   "metadata": {},
   "source": [
    "A general limitation of vector indexes is they don’t provide the ability to aggregate information like you would be using a structured query language like Cypher. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81c87e8f-6113-407a-b365-50f0f1f200f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How many open tickets are there?',\n",
       " 'result': 'There are 3 open tickets: Update, RecommendationFeature, and FeatureAdd. The 4th ticket, Refactor, is completed.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_qa.invoke(\n",
    "    \"How many open tickets are there?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc98f9-90bd-4535-8a36-4c5d23092903",
   "metadata": {},
   "source": [
    "The response seems valid, in part because the LLM uses assertive language. However, the response directly correlates to the number of retrieved documents from the vector index, which is three by default. So when the vector index retrieves three open tickets, the LLM unquestioningly believes there are no additional open tickets. However, we can validate whether this search result is true or not using a Cypher statement as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e41e361-3c46-4914-bb01-64e5cb553c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(*)': 5}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\n",
    "    \"MATCH (t:Task {status:'open'}) RETURN count(*)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0902e27-8b8a-4423-b769-3eb97266ab10",
   "metadata": {},
   "source": [
    "There are actually five open tasks in our toy graph. Vector similarity search is excellent for sifting through relevant information in unstructured text, but lacks the capability to analyse and aggregate structured information. Using Neo4j, this problem is easily solved by employing Cypher, a structured query language for graph databases. We need to explore whether LLM can translate our natural language question to a Cypher query and then give us the right answer.\n",
    "\n",
    "## Graph Cypher Search\n",
    "The good news is LangChain provides a GraphCypherQAChain, which generates the Cypher queries for you, so you don’t have to learn Cypher syntax to retrieve information from a graph database like Neo4j.\n",
    "\n",
    "The following code will refresh the graph schema and instantiate the Cypher chain. Generating valid Cypher statement according to the natural language prompt is a complex task. Therefore, it is recommended to use state-of-the-art LLMs like llama3.1. If you try llama3.0, you will see that it cannot understand the question properly and generate a wrong Cypher query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bac1dc52-9150-4739-9698-4586d2b7860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm = ChatGroq(temperature=0, model_name='llama-3.1-70b-versatile'),\n",
    "    qa_llm = ChatGroq(temperature=0), graph=graph, verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d34d0-bc1e-41aa-93a5-3c1b008f9504",
   "metadata": {},
   "source": [
    "Let's have a look at the results. It can be seen that it generates the exact Cypher query we expect and thus return the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8e6a6fb-3309-462c-8029-ae62d1879841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (t:Task {status: 'open'}) RETURN COUNT(t)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(t)': 5}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How many open tickets are there?',\n",
       " 'result': 'There are 5 open tickets.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.invoke(\n",
    "    \"How many open tickets are there?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe4f8b-fa8a-4f65-a8ba-fcf07d1ff69e",
   "metadata": {},
   "source": [
    "We can also ask the chain to aggregate the data using various grouping keys, like the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8d0ed68-3247-42a2-ab4c-dacfa9fb67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (t:Team)-[:ASSIGNED_TO]-(ts:Task {status: 'open'}) \n",
      "RETURN t.name, COUNT(ts) AS count \n",
      "ORDER BY count DESC \n",
      "LIMIT 1\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'t.name': 'TeamA', 'count': 3}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which Team has the most open Tasks?',\n",
       " 'result': 'TeamA has the most open tasks, with a count of 3.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.invoke(\n",
    "    \"Which Team has the most open Tasks?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79666c54-8405-487a-8051-06269a4c0cf1",
   "metadata": {},
   "source": [
    "You might say these aggregations are not graph-based operations, and that’s correct. We can, of course, perform more graph-based operations like traversing the dependency graph of microservices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45e2ca57-e4ef-4ac6-8871-e237603dc9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Microservice {name: 'Database'})<-[:DEPENDS_ON]-(s:Microservice) RETURN s.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'s.name': 'CatalogService'}, {'s.name': 'OrderService'}, {'s.name': 'UserService'}, {'s.name': 'PaymentService'}, {'s.name': 'InventoryService'}, {'s.name': 'AuthService'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which services depend on Database directly?',\n",
       " 'result': 'Based on the information provided, CatalogService, OrderService, UserService, PaymentService, InventoryService, and AuthService all depend on the Database directly.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.invoke(\n",
    "    \"Which services depend on Database directly?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e5d07-7b7b-4a7c-8132-32ad2a1f85a2",
   "metadata": {},
   "source": [
    "Of course, you can also ask the chain to produce variable-length path traversals by asking questions like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91ea8db5-6e2c-4d9d-837b-055ae7706945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH p=(m:Microservice {name: 'Database'})<-[:DEPENDS_ON*2..]-(s:Microservice) RETURN DISTINCT s\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'s': {'name': 'ShippingService', 'technology': 'Python'}}, {'s': {'name': 'OrderService', 'technology': 'Python'}}, {'s': {'name': 'PaymentService', 'technology': 'Node.js'}}, {'s': {'name': 'UserService', 'technology': 'Go'}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which services depend on Database indirectly?',\n",
       " 'result': \"I don't have enough information to determine which services depend on a database indirectly. However, I can tell you that ShippingService and OrderService use Python as their technology, PaymentService uses Node.js, and UserService uses Go.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.invoke(\n",
    "    \"Which services depend on Database indirectly?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9d516-5e68-480e-8962-582fa2c37787",
   "metadata": {},
   "source": [
    "Some of the mentioned services are the same as in the directly dependent question. The reason is the structure of the dependency graph and not the invalid Cypher statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e30d9-13d0-4818-ab0b-0fdaaa8de951",
   "metadata": {},
   "source": [
    "## Knowledge Graph Agent\n",
    "We’ve implemented separate tools for the structured and unstructured parts of the knowledge graph. Now we can add an agent to use these tools to explore the knowledge graph. In the code below, it is critical to set up the prompt template properly. For example, in \"Question\", we have to add \"ignore the question or query in the parse-able action\", and in \"Observation\", we have to add \"ignore the query part\". Otherwise, the internal reasoning will repeat the same thing again and again even the agent can get the final answer after the first iteration. This is because the agent thinks the question appearing in the parse-able action needs to be answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d3b306b-da35-4ac3-bcb1-7e26cc0c6c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the team assigned to maintain PaymentService, I need to look at the dependencies and assigned people for the microservice. This can be done using the Graph action.\n",
      "\n",
      "Action: Graph\n",
      "Action Input: {\"question\": \"Which team is assigned to maintain PaymentService?\"}\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (ms:Microservice {name: 'PaymentService'})-[:MAINTAINED_BY]->(t:Team) RETURN t.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'t.name': 'TeamD'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{'query': '{\"question\": \"Which team is assigned to maintain PaymentService?\"}', 'result': 'TeamD is assigned to maintain PaymentService.'}\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: TeamD is assigned to maintain PaymentService.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response: {'input': 'Which team is assigned to maintain PaymentService?', 'output': 'TeamD is assigned to maintain PaymentService.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, Tool, create_react_agent\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Tasks\",\n",
    "        func=vector_qa.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about descriptions of tasks.\n",
    "        Not useful for counting the number of tasks.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Graph\",\n",
    "        func=cypher_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about microservices,\n",
    "        their dependencies or assigned people. Also useful for any sort of \n",
    "        aggregation like counting the number of tasks, etc.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer, ignore the question or query in the parse-able action\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action, ignore the query part\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name='llama-3.1-70b-versatile')\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt, stop_sequence=True)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=2)\n",
    "response = agent_executor.invoke({\"input\": \"Which team is assigned to maintain PaymentService?\"})\n",
    "print(\"response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef529cde-a57d-4f9d-b71e-8c2176c11845",
   "metadata": {},
   "source": [
    "Now let's try to invoke the Tasks tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d8f73ba-8090-473d-bd6e-bcf1e2d7467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to find tasks that have the word \"optimization\" in their description. This requires searching through task descriptions, which is best handled by the Tasks function.\n",
      "\n",
      "Action: Tasks\n",
      "Action Input: {\"query\": \"What tasks have optimization in their description?\"}\u001b[0m\u001b[36;1m\u001b[1;3m{'query': '{\"query\": \"What tasks have optimization in their description?\"}', 'result': 'Based on the given context, the task with optimization in its description is:\\n\\n1. Refactor - Refactor the UserService codebase to enhance its readability, maintainability, and scalability, focusing primarily on modularization and optimization of existing functionalities.'}\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: Based on the given context, the task with optimization in its description is: Refactor - Refactor the UserService codebase to enhance its readability, maintainability, and scalability, focusing primarily on modularization and optimization of existing functionalities.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response: {'input': 'What tasks have optimization in their description?', 'output': 'Based on the given context, the task with optimization in its description is: Refactor - Refactor the UserService codebase to enhance its readability, maintainability, and scalability, focusing primarily on modularization and optimization of existing functionalities.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"What tasks have optimization in their description?\"})\n",
    "print(\"response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b915f2e-0ffb-4a06-893e-9fc4c3d79b15",
   "metadata": {},
   "source": [
    "If we compare the results above with those in this [neo4j blog](https://neo4j.com/developer-blog/knowledge-graph-rag-application/), we can see that the open source LLM either gives the same good results (the agent using Graph tool) or better one (the agent using Tasks tool). We consider the answer about the task is better because the llama model checks the exact word \"optimization\", while the gpt-4 model checks the word \"optimize\" and ignores \"optimization\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84168ef1-14d9-4f67-9be4-9b32640579b0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, this notebook has demonstrated how to use open source LLM and knowledge graph to build RAG applications and AI agents. The performance of llama3.1 is shown to be very good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
